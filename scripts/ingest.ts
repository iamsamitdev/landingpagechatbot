import { PDFLoader } from "@langchain/community/document_loaders/fs/pdf"
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters"
import { OpenAIEmbeddings } from "@langchain/openai"
import { Document } from "@langchain/core/documents"
import { neon } from "@neondatabase/serverless"
import fs from "fs"
import path from "path"
import dotenv from "dotenv"

// à¹‚à¸«à¸¥à¸” environment variables
dotenv.config({ path: ".env.local" })
dotenv.config({ path: ".env" })

// à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š environment variables
const databaseUri = process.env.DATABASE_URI
const openaiApiKey = process.env.OPENAI_API_KEY

if (!databaseUri) {
  console.error("âŒ Missing DATABASE_URI in environment variables")
  process.exit(1)
}

if (!openaiApiKey) {
  console.error("âŒ Missing OPENAI_API_KEY in environment variables")
  process.exit(1)
}

// à¸ªà¸£à¹‰à¸²à¸‡ Neon SQL Client
const sql = neon(databaseUri)

const DOCUMENTS_PATH = "./documents" // à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¹€à¸à¹‡à¸šà¹€à¸­à¸à¸ªà¸²à¸£

async function ingestDocuments() {
  console.log("ğŸš€ Starting document ingestion...")
  console.log(`ğŸ“‚ Looking for documents in: ${path.resolve(DOCUMENTS_PATH)}`)

  // à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸¡à¸µà¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ documents à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
  if (!fs.existsSync(DOCUMENTS_PATH)) {
    fs.mkdirSync(DOCUMENTS_PATH, { recursive: true })
    console.log("ğŸ“ Created documents folder. Please add PDF or TXT files and run again.")
    return
  }

  // 1. à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¹ƒà¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ
  const files = fs.readdirSync(DOCUMENTS_PATH)
  const pdfFiles = files.filter(f => f.endsWith(".pdf"))
  const txtFiles = files.filter(f => f.endsWith(".txt"))
  
  if (pdfFiles.length === 0 && txtFiles.length === 0) {
    console.log("âš ï¸ No PDF or TXT files found in documents folder.")
    return
  }

  console.log(`ğŸ“š Found ${pdfFiles.length} PDF files and ${txtFiles.length} TXT files`)

  const allDocs: Document[] = []

  // à¹‚à¸«à¸¥à¸” PDF files
  for (const file of pdfFiles) {
    const filePath = path.join(DOCUMENTS_PATH, file)
    console.log(`ğŸ“„ Loading PDF: ${file}`)
    
    try {
      const loader = new PDFLoader(filePath)
      const docs = await loader.load()
      
      docs.forEach(doc => {
        doc.metadata = { ...doc.metadata, source: file, type: "pdf" }
      })
      
      allDocs.push(...docs)
      console.log(`   âœ… Loaded ${docs.length} pages from ${file}`)
    } catch (error) {
      console.error(`   âŒ Error loading ${file}:`, error)
    }
  }

  // à¹‚à¸«à¸¥à¸” TXT files
  for (const file of txtFiles) {
    const filePath = path.join(DOCUMENTS_PATH, file)
    console.log(`ğŸ“„ Loading TXT: ${file}`)
    
    try {
      const content = fs.readFileSync(filePath, "utf-8")
      const doc = new Document({
        pageContent: content,
        metadata: { source: file, type: "txt" },
      })
      allDocs.push(doc)
      console.log(`   âœ… Loaded ${file}`)
    } catch (error) {
      console.error(`   âŒ Error loading ${file}:`, error)
    }
  }

  if (allDocs.length === 0) {
    console.log("âŒ No documents could be loaded.")
    return
  }

  console.log(`\nğŸ“š Total documents loaded: ${allDocs.length}`)

  // 2. à¹à¸šà¹ˆà¸‡à¹€à¸­à¸à¸ªà¸²à¸£à¹€à¸›à¹‡à¸™ Chunks
  console.log("\nâœ‚ï¸ Splitting documents into chunks...")
  const textSplitter = new RecursiveCharacterTextSplitter({
    chunkSize: 1000,
    chunkOverlap: 200,
  })

  const splitDocs = await textSplitter.splitDocuments(allDocs)
  console.log(`âœ… Created ${splitDocs.length} chunks`)

  // 3. à¸ªà¸£à¹‰à¸²à¸‡ Embeddings
  console.log("\nğŸ”„ Creating embeddings...")
  const embeddingsModel = new OpenAIEmbeddings({
    modelName: "text-embedding-3-small",
    openAIApiKey: openaiApiKey,
  })

  // 4. à¸¥à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸à¹ˆà¸²à¸­à¸­à¸à¸à¹ˆà¸­à¸™ (optional)
  console.log("ğŸ—‘ï¸ Clearing existing documents...")
  await sql`DELETE FROM documents WHERE id > 0`

  // 5. à¸šà¸±à¸™à¸—à¸¶à¸à¸¥à¸‡ Neon Postgres
  console.log("\nğŸ’¾ Saving to Neon Postgres...")
  let successCount = 0
  let errorCount = 0

  for (let i = 0; i < splitDocs.length; i++) {
    const doc = splitDocs[i]
    
    try {
      // à¸ªà¸£à¹‰à¸²à¸‡ embedding à¸ªà¸³à¸«à¸£à¸±à¸š chunk à¸™à¸µà¹‰
      const embedding = await embeddingsModel.embedQuery(doc.pageContent)

      // à¹à¸›à¸¥à¸‡ embedding array à¹€à¸›à¹‡à¸™ pgvector format string: [0.1,0.2,0.3,...]
      const embeddingString = `[${embedding.join(",")}]`

      // à¸šà¸±à¸™à¸—à¸¶à¸à¸¥à¸‡ database à¸œà¹ˆà¸²à¸™ Neon SQL
      await sql`
        INSERT INTO documents (content, metadata, embedding)
        VALUES (${doc.pageContent}, ${JSON.stringify(doc.metadata)}, ${embeddingString}::vector)
      `
      
      successCount++
      
      // à¹à¸ªà¸”à¸‡ progress à¸—à¸¸à¸ 10 chunks
      if ((i + 1) % 10 === 0 || i === splitDocs.length - 1) {
        const progress = Math.round(((i + 1) / splitDocs.length) * 100)
        console.log(`   ğŸ“Š Progress: ${i + 1}/${splitDocs.length} (${progress}%)`)
      }
    } catch (error) {
      console.error(`âŒ Error saving chunk ${i + 1}:`, error)
      errorCount++
    }
  }

  console.log("\n" + "=".repeat(50))
  console.log("ğŸ‰ Document ingestion completed!")
  console.log(`âœ… Successfully saved: ${successCount} chunks`)
  if (errorCount > 0) {
    console.log(`âŒ Failed: ${errorCount} chunks`)
  }
  console.log("=".repeat(50))
}

ingestDocuments().catch(error => {
  console.error("âŒ Fatal error:", error)
  process.exit(1)
})